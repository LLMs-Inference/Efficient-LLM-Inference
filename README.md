# 🚀 Efficient-LLM-Inference

Efficient Inference for Large Language Models

Once the swallows of the Wang and Xie halls, now they fly into the homes of ordinary people. This may be the very essence of efficient inference of big models: by optimizing LLM inference, pushing performance and efficiency to their limits, making big models no longer the exclusive domain of a select few (the wealthy, tech giants).


## 🚀 课程

- [MIT Song Han | Model Compression and Acceleration Techniques for AI Computing](https://efficientml.ai/)

- [Stanford NLP | CS336: Language Modeling from Scratch](https://stanford-cs336.github.io/spring2025/)

- [University of Pennsylvania | CIS 7000: Large Language Models](https://llm-class.github.io/)

- [UC Berkeley & Google | Large Language Model Agents](https://llmagents-learning.org/f24)

- [California Institute of Technology | Large Language Models for Reasoning](https://sites.google.com/view/cs-159-2024)

- [Washington University | CSE 561A: Large Language Models (2024 Fall)](https://teapot123.github.io/CSE561A_2024fl/)


## 🎮 视频教程

- [ZOMI 酱在 B 站上传了一系列视频教程](https://space.bilibili.com/517221395)

- [一小时精讲高性能 LLM 推理框架及细节优化](https://www.bilibili.com/video/BV1oT42117gL/)

- [Meta AI 的研究科学家 Kai Sheng Tai | 稀疏性促进高效 LLM 推理](https://www.youtube.com/watch?v=lIuHPxsgymU)

- [英伟达机器学习和深度学习专家 Christian Merkwirth | 优化 LLM 推理：挑战与最佳实践](https://www.youtube.com/watch?v=f7XcHUwQl4Y)

- [英伟达高级数据科学家 Mark Moyou | 从理论到具有成本效益的部署，掌握 LLM 推理优化](https://www.youtube.com/watch?v=9tvJ_GYJA-o)

- [NVIDIA GTC 技术干货 | 超大型 Transformer 模型的高效推理](https://www.nvidia.cn/on-demand/session/gtcspring23-s51088/)

- [硅谷知名华人 AI 科学家田渊栋 | 支持长文本上下文的 LLMs 的高效推理](https://www.youtube.com/watch?v=eXPhvQgAT_I)

- [Julien Simon 曾任亚马逊（AWS）人工智能和机器学习全球布道师 | 深度探究：LLM 推理优化](https://www.youtube.com/watch?v=hMs8VNRy5Ys)

- [OneFlow - 大语言模型的推理](https://www.bilibili.com/video/BV1pH4y1B7tY/)

- [KTransformers 团队分享异构推理架构思路：基于内存的大模型推理成本优化](https://www.bilibili.com/video/BV1VNQrYGEad/)


## 🛞 开源项目

开源项目：

- [vLLM - Easy, Fast, and Cheap LLM Serving for Everyone](https://github.com/vllm-project/vllm)

- [SGLang - A Fast Serving Framework for Large Language Models and Vision Language Models](https://github.com/sgl-project/sglang)

- [DeepSeek - The Path to Open-Sourcing the DeepSeek Inference Engine](https://github.com/deepseek-ai/open-infra-index)

- [HUAWEI Ascend MindIE](https://www.hiascend.com/software/mindie)

- [llama.cpp - LLM Inference in C/C++](https://github.com/ggml-org/llama.cpp)

- [stable-diffusion.cpp - Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All Devices](https://github.com/SealAILab/stable-diffusion-cpp)


## 📝 博客

- [结合 MindIE-LLM 框架的具体优化案例，分析大模型推理加速的关键技术](https://mp.weixin.qq.com/s/3QYQDq4ZHQRwYMs6MmgVLg)

- [两万六千字，大模型核心技术综述：微调、推理与优化指南](https://mp.weixin.qq.com/s/TCG_dDhoUvtlmtcO_dwSgw)

- [大模型推理服务全景图](https://mp.weixin.qq.com/s/cDELflSEM7SV2Z3bupy65g)

- [Lilian Weng 出品，必是精品 | 大型 Transformer 模型推理优化](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

- [高性能 LLM 推理框架的设计与实现](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

- [AI 推理场景的痛点和解决方案](https://mp.weixin.qq.com/s/SeUJxNK10fhR6YsWSJRYwg)

- [Semi-PD，P/D 半分离的调度策略，大模型推理范式新选择](https://mp.weixin.qq.com/s/vQ5iXCXD7lJXogvT52PsLg)

- [一念 LLM 大语言模型推理加速](https://mp.weixin.qq.com/s/bmafuEaB3pfG72xEaPcR3g)

- [2024 年 - 开源大模型推理引擎现状及常见推理优化方法](https://zhuanlan.zhihu.com/p/755874470)

- [大模型推理引擎国产化思考和实践](https://mp.weixin.qq.com/s/JLeYxYXurHbtF0nL-2wYyw)

- [大模型的模型压缩与有效推理要点总结](https://mp.weixin.qq.com/s/8AltJXjXIZHvq7lPu8FKoQ)

- [「乘法变加法」！MIT 清华校友全新方法优化 Transformer：Addition is All You Need](https://mp.weixin.qq.com/s/LdRacBGfjyF8xUJP6h6xbw)


## 💻 方法论文

- [Li, Y., Wei, F., Zhang, C., & Zhang, H. (2025). EAGLE-3: Scaling Up Inference Acceleration of Large Language Models via Training-Time Test. arXiv preprint arXiv: 2503.01840.](https://arxiv.org/abs/2503.01840)

- [Zhang, T., Sui, Y., Zhong, S., Chaudhary, V., Hu, X., & Shrivastava, A. (2025). 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. arXiv preprint arXiv: 2504.11651.](https://arxiv.org/abs/2504.11651)

- [Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., ... & Ceze, L. (2025). FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving. arXiv preprint arXiv: 2501.01005.](https://arxiv.org/abs/2501.01005)

- [Prabhu, R., Nayak, A., Mohan, J., Ramjee, R., & Panwar, A. (2025). vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 1: 1133-1150.](https://doi.org/10.1145/3669940.3707256)

- [Huang, Y., Wan, L. J., Ye, H., Jha, M., Wang, J., Li, Y., ... & Chen, D. (2024). Invited: New Solutions on LLM Acceleration, Optimization, and Application. In Proceedings of the 61st ACM/IEEE Design Automation Conference (DAC), 1-4.](https://doi.org/10.1145/3649329.3663517)

- [Chen, Y., & Huang, G. (2024). GUIDE: A Global Unified Inference Engine for Deploying Large Language Models in Heterogeneous Environments. arXiv preprint arXiv: 2412.04788.](https://arxiv.org/abs/2412.04788)

- [Dong, J., Feng, B., Guessous, D., Liang, Y., & He, H. (2024). Flex Attention: A Programming Model for Generating Optimized Attention Kernels. arXiv preprint arXiv: 2412.05496.](https://arxiv.org/abs/2412.05496)

- [Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., ... & Stoica, I. (2023). Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP), 611-626.](https://doi.org/10.1145/3600006.3613165)


## 💡 综述论文

- [Zhen, R., Li, J., Ji, Y., Yang, Z., Liu, T., Xia, Q., ... & Zhang, M. (2025). Taming the Titans: A Survey of Efficient LLM Inference Serving. arXiv preprint arXiv: 2504.19720.](https://arxiv.org/abs/2504.19720)

- [Park, S., Jeon, S., Lee, C., Jeon, S., Kim, B. S., & Lee, J. (2025). A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency. arXiv preprint arXiv: 2505.01658.](https://arxiv.org/abs/2505.01658)

- [Liu, Y., Wu, J., He, Y., Gao, H., Chen, H., Bi, B., ... & Hooi, B. (2025). Efficient Inference for Large Reasoning Models: A Survey. arXiv preprint arXiv: 2503.23077.](https://arxiv.org/abs/2503.23077)

- [Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., ... & Wang, Y. (2024). A Survey on Efficient Inference for Large Language Models. arXiv preprint arXiv: 2404.14294.](https://arxiv.org/abs/2404.14294)

- [Chavan, A., Magazine, R., Kushwaha, S., Debbah, M., & Gupta, D. (2024). Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI), 7980-7988.](https://doi.org/10.24963/ijcai.2024/883)

- [Luohe, S., Zhang, H., Yao, Y., Li, Z & Zhao, H. (2024). Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption. In First Conference on Language Modeling (COLM).](https://openreview.net/forum?id=8tKjqqMM5z#discussion)

- [Li, H., Li, Y., Tian, A., Tang, T., Xu, Z., Chen, X., ... & Chen, L. (2024). A Survey on Large Language Model Acceleration Based on KV Cache Management. arXiv preprint arXiv: 2412.19442.](https://arxiv.org/abs/2412.19442)

- [Kim, S., Hooper, C., Wattanawong, T., Kang, M., Yan, R., Genc, H., ... & Gholami, A. Full Stack Optimization of Transformer Inference. In Architecture and System Support for Transformer Models (ASSYST@ ISCA 2023).](https://arxiv.org/abs/2302.14017)


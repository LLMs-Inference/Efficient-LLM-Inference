# Efficient-LLM-Inference

Efficient Inference for Large Language Models

## ğŸš€ è¯¾ç¨‹ï¼š

- [MIT Song Han | Model Compression and Acceleration Techniques for AI Computing](https://efficientml.ai/)

- [University of Pennsylvania | CIS 7000 - Large Language Models](https://llm-class.github.io/)

- [UC Berkeley & Google - Large Language Model Agents](https://llmagents-learning.org/f24)

- [California Institute of Technology - Large Language Models for Reasoning](https://sites.google.com/view/cs-159-2024)

- [CSE 561A: Large Language Models (2024 Fall)](https://teapot123.github.io/CSE561A_2024fl/)


## ğŸ® è§†é¢‘æ•™ç¨‹ï¼š

- [ä¸€å°æ—¶ç²¾è®²é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶åŠç»†èŠ‚ä¼˜åŒ–](https://www.bilibili.com/video/BV1oT42117gL/)

- [Meta AI çš„ç ”ç©¶ç§‘å­¦å®¶ Kai Sheng Tai | ç¨€ç–æ€§ä¿ƒè¿›é«˜æ•ˆ LLM æ¨ç†](https://www.youtube.com/watch?v=lIuHPxsgymU)

- [è‹±ä¼Ÿè¾¾æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸“å®¶ Christian Merkwirth | ä¼˜åŒ– LLM æ¨ç†ï¼šæŒ‘æˆ˜ä¸æœ€ä½³å®è·µ](https://www.youtube.com/watch?v=f7XcHUwQl4Y)

- [è‹±ä¼Ÿè¾¾é«˜çº§æ•°æ®ç§‘å­¦å®¶ Mark Moyou | ä»ç†è®ºåˆ°å…·æœ‰æˆæœ¬æ•ˆç›Šçš„éƒ¨ç½²ï¼ŒæŒæ¡ LLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=9tvJ_GYJA-o)

- [NVIDIA GTC æŠ€æœ¯å¹²è´§ | è¶…å¤§å‹ Transformer æ¨¡å‹çš„é«˜æ•ˆæ¨ç†](https://www.nvidia.cn/on-demand/session/gtcspring23-s51088/)

- [ç¡…è°·çŸ¥ååäºº AI ç§‘å­¦å®¶ç”°æ¸Šæ ‹ | æ”¯æŒé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡çš„ LLMs çš„é«˜æ•ˆæ¨ç†](https://www.youtube.com/watch?v=eXPhvQgAT_I)

- [Julien Simon æ›¾ä»»äºšé©¬é€Šï¼ˆAWSï¼‰äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ å…¨çƒå¸ƒé“å¸ˆ | æ·±åº¦æ¢ç©¶ï¼šLLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=hMs8VNRy5Ys)


## ğŸ› å¼€æºé¡¹ç›®ï¼š

å¼€æºé¡¹ç›®ï¼š

- [Easy, Fast, and Cheap LLM Serving for Everyone](https://github.com/vllm-project/vllm)

- [SGLang is a Fast Serving Framework for Large Language Models and Vision Language Models](https://github.com/sgl-project/sglang)

- [The Path to Open-Sourcing the DeepSeek Inference Engine](https://github.com/deepseek-ai/open-infra-index)


## ğŸ“ åšå®¢ï¼š

- [ç»“åˆ MindIE-LLM æ¡†æ¶çš„å…·ä½“ä¼˜åŒ–æ¡ˆä¾‹ï¼Œåˆ†æå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿçš„å…³é”®æŠ€æœ¯](https://mp.weixin.qq.com/s/3QYQDq4ZHQRwYMs6MmgVLg)

- [Lilian Weng å‡ºå“ï¼Œå¿…æ˜¯ç²¾å“ | å¤§å‹ Transformer æ¨¡å‹æ¨ç†ä¼˜åŒ–](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

- [é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶çš„è®¾è®¡ä¸å®ç°](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

- [Semi-PDï¼ŒP/D åŠåˆ†ç¦»çš„è°ƒåº¦ç­–ç•¥ï¼Œå¤§æ¨¡å‹æ¨ç†èŒƒå¼æ–°é€‰æ‹©](https://mp.weixin.qq.com/s/vQ5iXCXD7lJXogvT52PsLg)

- [ä¸€å¿µ LLM å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿ](https://mp.weixin.qq.com/s/bmafuEaB3pfG72xEaPcR3g)


## ğŸ’» æ–¹æ³•è®ºæ–‡ï¼š

- [Zhang, T., Sui, Y., Zhong, S., Chaudhary, V., Hu, X., & Shrivastava, A. (2025). 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. arXiv preprint arXiv: 2504.11651.](https://arxiv.org/abs/2504.11651)


## ğŸ’¡ ç»¼è¿°è®ºæ–‡ï¼š

- [Kim S, Hooper C, Wattanawong T, et al. Full Stack Optimization of Transformer Inference\[C\]//Architecture and System Support for Transformer Models (ASSYST@ ISCA 2023).](https://arxiv.org/abs/2302.14017)


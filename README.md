# ğŸš€ Efficient-LLM-Inference


æ—§æ—¶ç‹è°¢å ‚å‰ç‡•ï¼Œé£å…¥å¯»å¸¸ç™¾å§“å®¶ã€‚è¿™æˆ–è®¸æ­£æ˜¯å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨ç†çš„æ„ä¹‰æ‰€åœ¨ï¼šé€šè¿‡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼Œå°†æ€§èƒ½å’Œæ•ˆç‡æå‡è‡³æè‡´ï¼Œè®©å¤§æ¨¡å‹ä¸å†åªæ˜¯å°‘æ•°äººï¼ˆå¯Œäººã€ç§‘æŠ€å·¨å¤´å…¬å¸ï¼‰çš„ä¸“å±ã€‚


## ğŸš€ è¯¾ç¨‹

- [MIT Song Han | Model Compression and Acceleration Techniques for AI Computing](https://efficientml.ai/)

- [Stanford NLP | CS336: Language Modeling from Scratch](https://stanford-cs336.github.io/spring2025/)

- [University of Pennsylvania | CIS 7000: Large Language Models](https://llm-class.github.io/)

- [UC Berkeley & Google | Large Language Model Agents](https://llmagents-learning.org/f24)

- [California Institute of Technology | Large Language Models for Reasoning](https://sites.google.com/view/cs-159-2024)

- [Washington University | CSE 561A: Large Language Models (2024 Fall)](https://teapot123.github.io/CSE561A_2024fl/)


## ğŸ® è§†é¢‘æ•™ç¨‹

- [ZOMI é…±åœ¨ B ç«™ä¸Šä¼ äº†ä¸€ç³»åˆ—è§†é¢‘æ•™ç¨‹](https://space.bilibili.com/517221395)

- [å¦‚ä½•åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼Ÿä¸‡å­—é•¿æ–‡ç»¼è¿°å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨ç†æŠ€æœ¯](https://www.bilibili.com/video/BV1yb421n7ZU/)

- [ICML-2024  Towards Efficient Generative Large Language Model Serving: A Tutorial from Algorithms to Systems](https://icml.cc/virtual/2024/tutorial/35229)

- [é«˜æ•ˆå¤§æ¨¡å‹æ¨ç†æŠ€æœ¯ä¸åº”ç”¨ | æ— é—®èŠ¯ç©¹é¦–å¸­ç§‘å­¦å®¶æˆ´å›½æµ© GenAICon 2025 æŠ¥å‘Š](https://www.bilibili.com/video/BV1hX7dzKEuA/)

- [å›½äº§å¤§ç®—åŠ› GPU å¦‚ä½•è¿æ¥ DeepSeek æ–°æœºé‡ | å£ä»ç§‘æŠ€ AI è½¯ä»¶é¦–å¸­æ¶æ„å¸ˆä¸äº‘å¸† GenAICon 2025 æ¼”è®²](https://www.bilibili.com/video/BV1S97dzvE1o/)

- [ä¸€äº›å·²æˆä¸º LLM æ¨ç†å¼•æ“ä¸­äº‹å®æ ‡å‡†çš„æ–¹æ³•](https://zhuanlan.zhihu.com/p/685706549)

- [ä¸€å°æ—¶ç²¾è®²é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶åŠç»†èŠ‚ä¼˜åŒ–](https://www.bilibili.com/video/BV1oT42117gL/)

- [Meta AI çš„ç ”ç©¶ç§‘å­¦å®¶ Kai Sheng Tai | ç¨€ç–æ€§ä¿ƒè¿›é«˜æ•ˆ LLM æ¨ç†](https://www.youtube.com/watch?v=lIuHPxsgymU)

- [è‹±ä¼Ÿè¾¾æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸“å®¶ Christian Merkwirth | ä¼˜åŒ– LLM æ¨ç†ï¼šæŒ‘æˆ˜ä¸æœ€ä½³å®è·µ](https://www.youtube.com/watch?v=f7XcHUwQl4Y)

- [è‹±ä¼Ÿè¾¾é«˜çº§æ•°æ®ç§‘å­¦å®¶ Mark Moyou | ä»ç†è®ºåˆ°å…·æœ‰æˆæœ¬æ•ˆç›Šçš„éƒ¨ç½²ï¼ŒæŒæ¡ LLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=9tvJ_GYJA-o)

- [åŒ—å¤§åˆ˜è­å“² | é¢å‘ AI Infra çš„ç³»ç»Ÿè½¯ä»¶ï¼šæŒ‘æˆ˜ã€æ€è€ƒå’Œå®è·µ](https://www.bilibili.com/video/BV1FF5XzNEux/)

- [NVIDIA GTC æŠ€æœ¯å¹²è´§ | è¶…å¤§å‹ Transformer æ¨¡å‹çš„é«˜æ•ˆæ¨ç†](https://www.nvidia.cn/on-demand/session/gtcspring23-s51088/)

- [ç¡…è°·çŸ¥ååäºº AI ç§‘å­¦å®¶ç”°æ¸Šæ ‹ | æ”¯æŒé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡çš„ LLMs çš„é«˜æ•ˆæ¨ç†](https://www.youtube.com/watch?v=eXPhvQgAT_I)

- [Julien Simon æ›¾ä»»äºšé©¬é€Šï¼ˆAWSï¼‰äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ å…¨çƒå¸ƒé“å¸ˆ | æ·±åº¦æ¢ç©¶ï¼šLLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=hMs8VNRy5Ys)

- [OneFlow - å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†](https://www.bilibili.com/video/BV1pH4y1B7tY/)

- [KTransformers å›¢é˜Ÿåˆ†äº«å¼‚æ„æ¨ç†æ¶æ„æ€è·¯ï¼šåŸºäºå†…å­˜çš„å¤§æ¨¡å‹æ¨ç†æˆæœ¬ä¼˜åŒ–](https://www.bilibili.com/video/BV1VNQrYGEad/)


## ğŸ› å¼€æºé¡¹ç›®

å¼€æºé¡¹ç›®ï¼š

- [vLLM - Easy, Fast, and Cheap LLM Serving for Everyone](https://github.com/vllm-project/vllm)

- [SGLang - A Fast Serving Framework for Large Language Models and Vision Language Models](https://github.com/sgl-project/sglang)

- [Nano-vLLM - A Lightweight vLLM Implementation Built from Scratch](https://github.com/GeeeekExplorer/nano-vllm/)

- [llm-d: A Kubernetes-Native High-Performance Distributed LLM Inference Framework](https://github.com/llm-d)

- [NVIDIA - LM Studio](https://github.com/lmstudio-ai)

- [HUAWEI Ascend MindIE](https://www.hiascend.com/software/mindie)

- [DeepSeek - The Path to Open-Sourcing the DeepSeek Inference Engine](https://github.com/deepseek-ai/open-infra-index)

- [RTP-LLM: Alibaba's High-Performance LLM Inference Engine for Diverse Applications](https://github.com/alibaba/rtp-llm)

- [Chitu - High-Performance Inference Framework for Large Language Models, Focusing on Efficiency, Flexibility, and Availability.](https://github.com/thu-pacman/chitu)

- [llama.cpp - LLM Inference in C/C++](https://github.com/ggml-org/llama.cpp)

- [stable-diffusion.cpp - Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All Devices](https://github.com/SealAILab/stable-diffusion-cpp)


## ğŸ“ åšå®¢

- [ä¿æ˜Ÿå‡¯ Nano-vLLMï¼šä»é›¶å¼€å§‹æ„å»ºäº†ä¸€ä¸ªè½»é‡çº§ vLLM å®ç°ï¼Œå°†ä»£ç ç®€åŒ–åˆ°äº† 1200 è¡Œ](https://mp.weixin.qq.com/s/Qo0xE6BtG4RpMIaeWCWsZQ)

- [å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯ç»¼è¿°ï¼ˆThe Art of LLM Inferenceï¼‰](https://mp.weixin.qq.com/s/NGrY5hD_DM5GH1bx2Id5Rg)

- [æ˜‡è…¾ Ã— ç›˜å¤ï¼šæ¨¡å‹ä¸ç³»ç»Ÿå…¨é“¾è·¯ä¼˜åŒ–ï¼Œæ‰“é€ å¤§æ¨¡å‹æ¨ç† "æ€§èƒ½ç‹ç‚¸" ç»„åˆï¼](https://mp.weixin.qq.com/s/JisTVFKmeo2pnLNvtEfhoQ)

- [è°·æ­Œä¹‹åï¼Œè‹±ä¼Ÿè¾¾å…¥å±€æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹ï¼ŒFast-dLLM æ¨ç†é€Ÿåº¦ç‹‚é£™ 27.6 å€](https://mp.weixin.qq.com/s/b-9yl8tT5IxIrUBtUp8WOg)

- [å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ¡†æ¶çš„å…¨é¢åˆ†æä¸é€‰å‹æŒ‡å—ï¼ˆ2025 å¹´ç‰ˆï¼‰](https://mp.weixin.qq.com/s/vadmuBp4TVr7Lug-7ALyCg)

- [å‘Šåˆ«å¡è„–å­ï¼Œåä¸ºé»‘ç§‘æŠ€ç ´å±€ï¼æ˜‡è…¾æ¨ç†åŠ é€Ÿ 1.6 å€æ‰“ç ´ LLM é™æ™ºé­”å’’](https://mp.weixin.qq.com/s/u6W4Fmv1r836iR53uA00oA)

- [æ˜‡è…¾è¶…å¤§è§„æ¨¡ MoE æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯æ­ç§˜ï¼šMTP è°ƒåº¦ 10 å€æé€Ÿï¼ŒINT æ‰“å¹³ FP8](https://mp.weixin.qq.com/s/aPf4Ik1Ts0EKEiquzK1yCQ)

- [å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æ¦‚è¿°](https://mp.weixin.qq.com/s/P_zgu4NixEyjjq00zoQ8bg)

- [ICLR 2025 | è®¡ç®—å¼€é”€å‡åŠï¼Dynamic-LLaVA åˆ·æ–°å¤šæ¨¡æ€å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿä¸Šé™](https://mp.weixin.qq.com/s/2SbbAlLxZG9Ub_lZf8YVDw)

- [å¸®å¤§æ¨¡å‹æé€Ÿ 80%ï¼Œåä¸ºæ‹¿å‡ºæ˜‡è…¾æ¨ç†æ€æ‰‹é” FlashCommï¼Œä¸‰æ‹›æå®šé€šç®—ç“¶é¢ˆ](https://mp.weixin.qq.com/s/-20G3l14552RiENOOgxgtg)

- [å¦‚ä½•é‡ç° DeepSeek æ¨ç†æ€§èƒ½çªç ´](https://mp.weixin.qq.com/s/ktG7e9PgHhzwIDmaG7XvpQ)

- [åä¸º + DeepSeekï¼Œæ¨ç†æ€§èƒ½åˆ›æ–°é«˜ï¼æŠ€æœ¯æŠ¥å‘Šä¹Ÿå…¬å¸ƒå‡ºæ¥äº†](https://mp.weixin.qq.com/s/UzXNIFesgBcMtfetgp2Y7Q)

- [åä¸ºæ›å…‰ä¸¤å¤§é»‘ç§‘æŠ€ï¼æ‰“ç ´æ¨ç†å»¶è¿Ÿé­”å’’ï¼Œå¤§æ¨¡å‹ä»æ­¤ã€Œç§’å›ã€](https://mp.weixin.qq.com/s/P6MYUJf_0gEu8hbh1KVvEA)

- [åä¸ºæ˜¯æ€ä¹ˆè®©å¤§æ¨¡å‹æé€Ÿçš„ï¼Ÿ](https://mp.weixin.qq.com/s/QZl0OvhA8XoXOaTC6dqxWw)

- [ä¸€ä½œè§£è¯»ï¼ä» idea è§†è§’ï¼ŒèŠèŠ Qwen æ¨å‡ºçš„æ–° Scaling Law â€”â€” Parallel Scaling](https://mp.weixin.qq.com/s/E4G2HwqQ47RTtPInOPaFMA)

- [å¶å­è±ªã€é™ˆå¤©å¥‡ç­‰äººå¼€æºé¡¹ç›® FlashInfer å…¥é€‰ï¼ŒMLSys2025 æœ€ä½³è®ºæ–‡å¥–å…¬å¸ƒ](https://mp.weixin.qq.com/s/ifDaTpoo0VXCa6UutgSmMA)

- [ç»“åˆ MindIE-LLM æ¡†æ¶çš„å…·ä½“ä¼˜åŒ–æ¡ˆä¾‹ï¼Œåˆ†æå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿçš„å…³é”®æŠ€æœ¯](https://mp.weixin.qq.com/s/3QYQDq4ZHQRwYMs6MmgVLg)

- [å¤§æ¨¡å‹æ¨ç†åˆ©å™¨ï¼š3 å¤§å¼€æºæ¡†æ¶æ­ç§˜ï½œæ¼«è°ˆ MFUï¼ˆä¸‰ï¼‰](https://mp.weixin.qq.com/s/fK0td7LBVsV6bIxlEH2cMw)

- [DeepSeek-V3 å†å‘è®ºæ–‡ï¼Œæ¢ç¥ç½²åï¼Œæ¢è®¨æ‰©å±•æŒ‘æˆ˜å¹¶åæ€ç¡¬ä»¶æ¶æ„](https://mp.weixin.qq.com/s/vGeagqanhmtR0pX5Obm7rw)

- [å¤§æ¨¡å‹æ¨ç†æˆæœ¬æ¯å¹´é™ä½ 10 å€çš„ç§˜å¯†ï¼šä¸€æ–‡äº†è§£ vLLMã€SGLang ç­‰ä¸»æµæ¨ç†å¼•æ“](https://mp.weixin.qq.com/s/wyIIrDxYchROaAB6J38KPw)

- [æ¢æ–‡é”‹æ–°è®ºæ–‡ï¼DeepSeek é™æœ¬ç§˜ç±å…¬å¼€ï¼Œçªç ´ç®—åŠ›ç“¶é¢ˆæœ‰å…­æ‹›](https://mp.weixin.qq.com/s/DtPFxxJWMFkjTK3ApAOLqg)

- [æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹ LLM çš„æ¨ç†ä¼˜åŒ–ç§˜å¯†](https://mp.weixin.qq.com/s/C6nQwCcuEDjH0hIn9jfXow)

- [ä¸¤ä¸‡å…­åƒå­—ï¼Œå¤§æ¨¡å‹æ ¸å¿ƒæŠ€æœ¯ç»¼è¿°ï¼šå¾®è°ƒã€æ¨ç†ä¸ä¼˜åŒ–æŒ‡å—](https://mp.weixin.qq.com/s/TCG_dDhoUvtlmtcO_dwSgw)

- [æ¼«è°ˆå¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–æŠ€æœ¯ç³»åˆ— â€”â€”ï¼ˆäºŒï¼‰æ¨¡å‹æ¨ç†æœåŠ¡](https://mp.weixin.qq.com/s/3RBK_CR-YbmPqYClkEd1Lg)

- [2024 å¹´ - å¼€æºå¤§æ¨¡å‹æ¨ç†å¼•æ“ç°çŠ¶åŠå¸¸è§æ¨ç†ä¼˜åŒ–æ–¹æ³•](https://zhuanlan.zhihu.com/p/755874470)

- [10 ç§ä¸»æµ LLM æ¨ç†æ¡†æ¶çš„æŠ€æœ¯ä»‹ç»ä¸å¯¹æ¯”ï¼šä»æœ¬åœ°éƒ¨ç½²åˆ°ä¼ä¸šçº§æœåŠ¡](https://mp.weixin.qq.com/s/gfG1XQxamaFH5W9D0P4GpQ)

- [ä¸€æ–‡è¯»æ‡‚ llama.cpp/vLLM/SGLang/FastTransformer/TensorRT/TGI/MindIE å¤§æ¨¡å‹æ¨ç†å¼•æ“ï¼ˆé™„å›¾ï¼‰](https://mp.weixin.qq.com/s/FOwTIrDBQPk7DJf3CHI6eQ)

- [é˜¿é‡Œäº‘ Higress å›¢é˜Ÿ - å¤§æ¨¡å‹æ¨ç†æœåŠ¡å…¨æ™¯å›¾](https://mp.weixin.qq.com/s/cDELflSEM7SV2Z3bupy65g)

- [å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ– â€”â€” æŠ€æœ¯çªç ´ä¸äº§ä¸šè½åœ°æ–°èŒƒå¼](https://mp.weixin.qq.com/s/e434Y1DYPkLCfbIEg4qbrw)

- [æ¯«ç§’çº§å“åº”çš„ç§˜å¯†ï¼šè§£å¯†å¤§æ¨¡å‹æ¨ç†å±‚å¦‚ä½•ç”¨ 10% æˆæœ¬å®ç° 10 å€é€Ÿæ€§èƒ½](https://mp.weixin.qq.com/s/CguEVKJ2MtMdEeoclEGD8Q)

- [ä¸‡å­—é•¿æ–‡ï¼Œå¤§æ¨¡å‹æ¨ç†æ€§èƒ½ä¼˜åŒ–ç»¼è¿°æ±‡æ€»](https://mp.weixin.qq.com/s/ZGUjE8iqg4EpFBdijbThzQ)

- [è¯­è¨€å¤§æ¨¡å‹çš„æ¨ç†æŠ€å·§](https://mp.weixin.qq.com/s/0WeKGKuYCfxKx6eve7exwg)

- [AI æ¨ç†åœºæ™¯çš„ç—›ç‚¹å’Œè§£å†³æ–¹æ¡ˆ](https://mp.weixin.qq.com/s/SeUJxNK10fhR6YsWSJRYwg)

- [ä¸ƒç§å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿçš„æ–¹æ³•](https://mp.weixin.qq.com/s/8s8RK7uY3TwR7il1UjcOLQ)

- [Lilian Weng å‡ºå“ï¼Œå¿…æ˜¯ç²¾å“ | å¤§å‹ Transformer æ¨¡å‹æ¨ç†ä¼˜åŒ–](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

- [å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–å…³é”®æŠ€æœ¯](https://mp.weixin.qq.com/s/UtqilJ-WIvyzYxZOwlK_rQ)

- [é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶çš„è®¾è®¡ä¸å®ç°](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

- [ä¸€æ–‡è¯»æ‡‚å¤§æ¨¡å‹æ¨ç†æœåŠ¡å¹³å°ï¼ˆXinference/Ollama/GPUStack/KServe/Triton/LMDeployï¼‰](https://mp.weixin.qq.com/s/utzpNKbmIIFPPuKox8vPvg)

- [Semi-PDï¼ŒP/D åŠåˆ†ç¦»çš„è°ƒåº¦ç­–ç•¥ï¼Œå¤§æ¨¡å‹æ¨ç†èŒƒå¼æ–°é€‰æ‹©](https://mp.weixin.qq.com/s/vQ5iXCXD7lJXogvT52PsLg)

- [ä¸€å¿µ LLM å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿ](https://mp.weixin.qq.com/s/huL-EO7mhj8bI3dRZL2buw)

- [å¤§æ¨¡å‹æ¨ç†æ˜¾å­˜ä¼˜åŒ–çš„æ·±åº¦æ¢ç´¢ä¸å®è·µ](https://mp.weixin.qq.com/s/zJfJ7aZZ2BfQwq5rVwjt0Q)

- [å¤§æ¨¡å‹æ¨ç†å¼•æ“å›½äº§åŒ–æ€è€ƒå’Œå®è·µ](https://mp.weixin.qq.com/s/JLeYxYXurHbtF0nL-2wYyw)

- [å¤§æ¨¡å‹çš„æ¨¡å‹å‹ç¼©ä¸æœ‰æ•ˆæ¨ç†è¦ç‚¹æ€»ç»“](https://mp.weixin.qq.com/s/8AltJXjXIZHvq7lPu8FKoQ)

- [ã€Œä¹˜æ³•å˜åŠ æ³•ã€ï¼MIT æ¸…åæ ¡å‹å…¨æ–°æ–¹æ³•ä¼˜åŒ– Transformerï¼šAddition is All You Need](https://mp.weixin.qq.com/s/LdRacBGfjyF8xUJP6h6xbw)

- [è¿ˆå‘ 100 å€åŠ é€Ÿï¼šå…¨æ ˆ Transformer æ¨ç†ä¼˜åŒ–](https://mp.weixin.qq.com/s/1QlZ_d4BrAcD9YE9BEdyYg)



## ğŸ’» æ–¹æ³•è®ºæ–‡

- [Chen, M., Hui, B., Cui, Z., Yang, J., Liu, D., Sun, J., ... & Liu, Z. (2025). Parallel Scaling Law for Language Models. arXiv preprint arXiv: 2505.10475.](https://arxiv.org/abs/2505.10475)

- [Li, Y., Wei, F., Zhang, C., & Zhang, H. (2025). EAGLE-3: Scaling Up Inference Acceleration of Large Language Models via Training-Time Test. arXiv preprint arXiv: 2503.01840.](https://arxiv.org/abs/2503.01840)

- [Liao, B., Xu, Y., Dong, H., Li, J., Monz, C., Savarese, S., ... & Xiong, C. (2025). Reward-Guided Speculative Decoding for Efficient LLM Reasoning. arXiv preprint arXiv: 2501.19324.](https://arxiv.org/abs/2501.19324)

- [Zhang, T., Sui, Y., Zhong, S., Chaudhary, V., Hu, X., & Shrivastava, A. (2025). 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. arXiv preprint arXiv: 2504.11651.](https://arxiv.org/abs/2504.11651)

- [Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., ... & Ceze, L. (2025). FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving. arXiv preprint arXiv: 2501.01005.](https://arxiv.org/abs/2501.01005)

- [Xu, J., Pan, J., Zhou, Y., Chen, S., Li, J., Lian, Y., ... & Dai, G. (2025). SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting. arXiv preprint arXiv: 2504.08850.](https://arxiv.org/abs/2504.08850)

- [Prabhu, R., Nayak, A., Mohan, J., Ramjee, R., & Panwar, A. (2025). vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 1: 1133-1150.](https://doi.org/10.1145/3669940.3707256)

- [Huang, Y., Wan, L. J., Ye, H., Jha, M., Wang, J., Li, Y., ... & Chen, D. (2024). Invited: New Solutions on LLM Acceleration, Optimization, and Application. In Proceedings of the 61st ACM/IEEE Design Automation Conference (DAC), 1-4.](https://doi.org/10.1145/3649329.3663517)

- [Chen, Y., & Huang, G. (2024). GUIDE: A Global Unified Inference Engine for Deploying Large Language Models in Heterogeneous Environments. arXiv preprint arXiv: 2412.04788.](https://arxiv.org/abs/2412.04788)

- [Dong, J., Feng, B., Guessous, D., Liang, Y., & He, H. (2024). Flex Attention: A Programming Model for Generating Optimized Attention Kernels. arXiv preprint arXiv: 2412.05496.](https://arxiv.org/abs/2412.05496)

- [Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., ... & Stoica, I. (2023). Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP), 611-626.](https://doi.org/10.1145/3600006.3613165)


## ğŸ’¡ ç»¼è¿°è®ºæ–‡

- [Zhen, R., Li, J., Ji, Y., Yang, Z., Liu, T., Xia, Q., ... & Zhang, M. (2025). Taming the Titans: A Survey of Efficient LLM Inference Serving. arXiv preprint arXiv: 2504.19720.](https://arxiv.org/abs/2504.19720)

- [Park, S., Jeon, S., Lee, C., Jeon, S., Kim, B. S., & Lee, J. (2025). A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency. arXiv preprint arXiv: 2505.01658.](https://arxiv.org/abs/2505.01658)

- [Sui, Y., Chuang, Y. N., Wang, G., Zhang, J., Zhang, T., Yuan, J., ... & Hu, X. (2025). Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models. arXiv preprint arXiv: 2503.16419.](https://arxiv.org/abs/2503.16419)

- [è‘›æ—­å†‰, æ¬§æ´‹, ç‹åš, èµµå®‡, å´åˆ©èˆŸ, ç‹å­èª, é™ˆå¿—å¹¿, è‚–ä¾¬. å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„å­˜å‚¨ä¼˜åŒ–æŠ€æœ¯ç»¼è¿°\[J\]. è®¡ç®—æœºç ”ç©¶ä¸å‘å±•, 2025, 62(3): 545-562. DOI: 10.7544/issn1000-1239.202440628](https://crad.ict.ac.cn/article/doi/10.7544/issn1000-1239.202440628)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                

- [Liu, Y., Wu, J., He, Y., Gao, H., Chen, H., Bi, B., ... & Hooi, B. (2025). Efficient Inference for Large Reasoning Models: A Survey. arXiv preprint arXiv: 2503.23077.](https://arxiv.org/abs/2503.23077)

- [Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., ... & Wang, Y. (2024). A Survey on Efficient Inference for Large Language Models. arXiv preprint arXiv: 2404.14294.](https://arxiv.org/abs/2404.14294)

- [Li, J., Xu, J., Huang, S., Chen, Y., Li, W., Liu, J., ... & Dai, G. (2024). Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective. arXiv preprint arXiv: 2410.04466.](https://arxiv.org/abs/2410.04466)

- [Chavan, A., Magazine, R., Kushwaha, S., Debbah, M., & Gupta, D. (2024). Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI), 7980-7988.](https://doi.org/10.24963/ijcai.2024/883)

- [Luohe, S., Zhang, H., Yao, Y., Li, Z & Zhao, H. (2024). Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption. In First Conference on Language Modeling (COLM).](https://openreview.net/forum?id=8tKjqqMM5z#discussion)

- [Li, H., Li, Y., Tian, A., Tang, T., Xu, Z., Chen, X., ... & Chen, L. (2024). A Survey on Large Language Model Acceleration Based on KV Cache Management. arXiv preprint arXiv: 2412.19442.](https://arxiv.org/abs/2412.19442)

- [Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Liu, J., ... & Zhang, M. (2024). Efficient Large Language Models: A Survey. Transactions on Machine Learning Research (TMLR).](https://openreview.net/forum?id=bsCCJHbO8A)

- [Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Jin, H., Chen, T., & Jia, Z. (2023). Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems. arXiv preprint arXiv: 2312.15234.](https://arxiv.org/abs/2312.15234)

- [Kim, S., Hooper, C., Wattanawong, T., Kang, M., Yan, R., Genc, H., ... & Gholami, A. Full Stack Optimization of Transformer Inference. In Architecture and System Support for Transformer Models (ASSYST@ ISCA 2023).](https://arxiv.org/abs/2302.14017)


# Efficient-LLM-Inference

Efficient Inference for Large Language Models

## 🚀 课程：

- [MIT Song Han | Model Compression and Acceleration Techniques for AI Computing](https://efficientml.ai/)

- [University of Pennsylvania | CIS 7000 - Large Language Models](https://llm-class.github.io/)

- [UC Berkeley & Google - Large Language Model Agents](https://llmagents-learning.org/f24)

- [California Institute of Technology - Large Language Models for Reasoning](https://sites.google.com/view/cs-159-2024)

- [CSE 561A: Large Language Models (2024 Fall)](https://teapot123.github.io/CSE561A_2024fl/)


## 🎮 视频教程：

- [一小时精讲高性能 LLM 推理框架及细节优化](https://www.bilibili.com/video/BV1oT42117gL/)

- [Meta AI 的研究科学家 Kai Sheng Tai | 稀疏性促进高效 LLM 推理](https://www.youtube.com/watch?v=lIuHPxsgymU)

- [英伟达机器学习和深度学习专家 Christian Merkwirth | 优化 LLM 推理：挑战与最佳实践](https://www.youtube.com/watch?v=f7XcHUwQl4Y)

- [英伟达高级数据科学家 Mark Moyou | 从理论到具有成本效益的部署，掌握 LLM 推理优化](https://www.youtube.com/watch?v=9tvJ_GYJA-o)

- [NVIDIA GTC 技术干货 | 超大型 Transformer 模型的高效推理](https://www.nvidia.cn/on-demand/session/gtcspring23-s51088/)

- [硅谷知名华人 AI 科学家田渊栋 | 支持长文本上下文的 LLMs 的高效推理](https://www.youtube.com/watch?v=eXPhvQgAT_I)

- [Julien Simon 曾任亚马逊（AWS）人工智能和机器学习全球布道师 | 深度探究：LLM 推理优化](https://www.youtube.com/watch?v=hMs8VNRy5Ys)


## 🛞 开源项目：

开源项目：

- [Easy, Fast, and Cheap LLM Serving for Everyone](https://github.com/vllm-project/vllm)

- [SGLang is a Fast Serving Framework for Large Language Models and Vision Language Models](https://github.com/sgl-project/sglang)

- [The Path to Open-Sourcing the DeepSeek Inference Engine](https://github.com/deepseek-ai/open-infra-index)


## 📝 博客：

- [结合 MindIE-LLM 框架的具体优化案例，分析大模型推理加速的关键技术](https://mp.weixin.qq.com/s/3QYQDq4ZHQRwYMs6MmgVLg)

- [Lilian Weng 出品，必是精品 | 大型 Transformer 模型推理优化](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

- [高性能 LLM 推理框架的设计与实现](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

- [Semi-PD，P/D 半分离的调度策略，大模型推理范式新选择](https://mp.weixin.qq.com/s/vQ5iXCXD7lJXogvT52PsLg)

- [一念 LLM 大语言模型推理加速](https://mp.weixin.qq.com/s/bmafuEaB3pfG72xEaPcR3g)


## 💻 方法论文：

- [Zhang, T., Sui, Y., Zhong, S., Chaudhary, V., Hu, X., & Shrivastava, A. (2025). 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. arXiv preprint arXiv: 2504.11651.](https://arxiv.org/abs/2504.11651)


## 💡 综述论文：

- [Kim S, Hooper C, Wattanawong T, et al. Full Stack Optimization of Transformer Inference\[C\]//Architecture and System Support for Transformer Models (ASSYST@ ISCA 2023).](https://arxiv.org/abs/2302.14017)


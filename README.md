# Efficient-LLM-Inference

Efficient Inference for Large Language Models


## ğŸš€ è¯¾ç¨‹

- [MIT Song Han | Model Compression and Acceleration Techniques for AI Computing](https://efficientml.ai/)

- [University of Pennsylvania | CIS 7000 - Large Language Models](https://llm-class.github.io/)

- [UC Berkeley & Google - Large Language Model Agents](https://llmagents-learning.org/f24)

- [California Institute of Technology - Large Language Models for Reasoning](https://sites.google.com/view/cs-159-2024)

- [CSE 561A: Large Language Models (2024 Fall)](https://teapot123.github.io/CSE561A_2024fl/)


## ğŸ® è§†é¢‘æ•™ç¨‹

- [ä¸€å°æ—¶ç²¾è®²é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶åŠç»†èŠ‚ä¼˜åŒ–](https://www.bilibili.com/video/BV1oT42117gL/)

- [Meta AI çš„ç ”ç©¶ç§‘å­¦å®¶ Kai Sheng Tai | ç¨€ç–æ€§ä¿ƒè¿›é«˜æ•ˆ LLM æ¨ç†](https://www.youtube.com/watch?v=lIuHPxsgymU)

- [è‹±ä¼Ÿè¾¾æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸“å®¶ Christian Merkwirth | ä¼˜åŒ– LLM æ¨ç†ï¼šæŒ‘æˆ˜ä¸æœ€ä½³å®è·µ](https://www.youtube.com/watch?v=f7XcHUwQl4Y)

- [è‹±ä¼Ÿè¾¾é«˜çº§æ•°æ®ç§‘å­¦å®¶ Mark Moyou | ä»ç†è®ºåˆ°å…·æœ‰æˆæœ¬æ•ˆç›Šçš„éƒ¨ç½²ï¼ŒæŒæ¡ LLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=9tvJ_GYJA-o)

- [NVIDIA GTC æŠ€æœ¯å¹²è´§ | è¶…å¤§å‹ Transformer æ¨¡å‹çš„é«˜æ•ˆæ¨ç†](https://www.nvidia.cn/on-demand/session/gtcspring23-s51088/)

- [ç¡…è°·çŸ¥ååäºº AI ç§‘å­¦å®¶ç”°æ¸Šæ ‹ | æ”¯æŒé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡çš„ LLMs çš„é«˜æ•ˆæ¨ç†](https://www.youtube.com/watch?v=eXPhvQgAT_I)

- [Julien Simon æ›¾ä»»äºšé©¬é€Šï¼ˆAWSï¼‰äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ å…¨çƒå¸ƒé“å¸ˆ | æ·±åº¦æ¢ç©¶ï¼šLLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=hMs8VNRy5Ys)


## ğŸ› å¼€æºé¡¹ç›®

å¼€æºé¡¹ç›®ï¼š

- [Easy, Fast, and Cheap LLM Serving for Everyone](https://github.com/vllm-project/vllm)

- [SGLang is a Fast Serving Framework for Large Language Models and Vision Language Models](https://github.com/sgl-project/sglang)

- [The Path to Open-Sourcing the DeepSeek Inference Engine](https://github.com/deepseek-ai/open-infra-index)


## ğŸ“ åšå®¢

- [ç»“åˆ MindIE-LLM æ¡†æ¶çš„å…·ä½“ä¼˜åŒ–æ¡ˆä¾‹ï¼Œåˆ†æå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿçš„å…³é”®æŠ€æœ¯](https://mp.weixin.qq.com/s/3QYQDq4ZHQRwYMs6MmgVLg)

- [ä¸¤ä¸‡å…­åƒå­—ï¼Œå¤§æ¨¡å‹æ ¸å¿ƒæŠ€æœ¯ç»¼è¿°ï¼šå¾®è°ƒã€æ¨ç†ä¸ä¼˜åŒ–æŒ‡å—](https://mp.weixin.qq.com/s/TCG_dDhoUvtlmtcO_dwSgw)

- [å¤§æ¨¡å‹æ¨ç†æœåŠ¡å…¨æ™¯å›¾](https://mp.weixin.qq.com/s/cDELflSEM7SV2Z3bupy65g)

- [Lilian Weng å‡ºå“ï¼Œå¿…æ˜¯ç²¾å“ | å¤§å‹ Transformer æ¨¡å‹æ¨ç†ä¼˜åŒ–](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

- [é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶çš„è®¾è®¡ä¸å®ç°](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

- [AI æ¨ç†åœºæ™¯çš„ç—›ç‚¹å’Œè§£å†³æ–¹æ¡ˆ](https://mp.weixin.qq.com/s/SeUJxNK10fhR6YsWSJRYwg)

- [Semi-PDï¼ŒP/D åŠåˆ†ç¦»çš„è°ƒåº¦ç­–ç•¥ï¼Œå¤§æ¨¡å‹æ¨ç†èŒƒå¼æ–°é€‰æ‹©](https://mp.weixin.qq.com/s/vQ5iXCXD7lJXogvT52PsLg)

- [ä¸€å¿µ LLM å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿ](https://mp.weixin.qq.com/s/bmafuEaB3pfG72xEaPcR3g)

- [å¤§æ¨¡å‹çš„æ¨¡å‹å‹ç¼©ä¸æœ‰æ•ˆæ¨ç†è¦ç‚¹æ€»ç»“](https://mp.weixin.qq.com/s/8AltJXjXIZHvq7lPu8FKoQ)


## ğŸ’» æ–¹æ³•è®ºæ–‡

- [Li, Y., Wei, F., Zhang, C., & Zhang, H. (2025). EAGLE-3: Scaling Up Inference Acceleration of Large Language Models via Training-Time Test. arXiv preprint arXiv: 2503.01840.](https://arxiv.org/abs/2503.01840)

- [Zhang, T., Sui, Y., Zhong, S., Chaudhary, V., Hu, X., & Shrivastava, A. (2025). 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. arXiv preprint arXiv: 2504.11651.](https://arxiv.org/abs/2504.11651)


## ğŸ’¡ ç»¼è¿°è®ºæ–‡

- [Zhen, R., Li, J., Ji, Y., Yang, Z., Liu, T., Xia, Q., ... & Zhang, M. (2025). Taming the Titans: A Survey of Efficient LLM Inference Serving. arXiv preprint arXiv: 2504.19720.](https://arxiv.org/abs/2504.19720)

- [Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., ... & Wang, Y. (2024). A Survey on Efficient Inference for Large Language Models. arXiv preprint arXiv: 2404.14294.](https://arxiv.org/abs/2404.14294)

- [Liu, Y., Wu, J., He, Y., Gao, H., Chen, H., Bi, B., ... & Hooi, B. (2025). Efficient Inference for Large Reasoning Models: A Survey. arXiv preprint arXiv: 2503.23077.](https://arxiv.org/abs/2503.23077)

- [Kim S, Hooper C, Wattanawong T, et al. Full Stack Optimization of Transformer Inference\[C\]//Architecture and System Support for Transformer Models (ASSYST@ ISCA 2023).](https://arxiv.org/abs/2302.14017)

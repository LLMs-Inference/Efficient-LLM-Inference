# ğŸš€ Efficient-LLM-Inference

Efficient Inference for Large Language Models

Once the swallows of the Wang and Xie halls, now they fly into the homes of ordinary people. This may be the very essence of efficient inference of big models: by optimizing LLM inference, pushing performance and efficiency to their limits, making big models no longer the exclusive domain of a select few (the wealthy, tech giants).


## ğŸš€ è¯¾ç¨‹

- [MIT Song Han | Model Compression and Acceleration Techniques for AI Computing](https://efficientml.ai/)

- [Stanford NLP | CS336: Language Modeling from Scratch](https://stanford-cs336.github.io/spring2025/)

- [University of Pennsylvania | CIS 7000: Large Language Models](https://llm-class.github.io/)

- [UC Berkeley & Google | Large Language Model Agents](https://llmagents-learning.org/f24)

- [California Institute of Technology | Large Language Models for Reasoning](https://sites.google.com/view/cs-159-2024)

- [Washington University | CSE 561A: Large Language Models (2024 Fall)](https://teapot123.github.io/CSE561A_2024fl/)


## ğŸ® è§†é¢‘æ•™ç¨‹

- [ZOMI é…±åœ¨ B ç«™ä¸Šä¼ äº†ä¸€ç³»åˆ—è§†é¢‘æ•™ç¨‹](https://space.bilibili.com/517221395)

- [ä¸€å°æ—¶ç²¾è®²é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶åŠç»†èŠ‚ä¼˜åŒ–](https://www.bilibili.com/video/BV1oT42117gL/)

- [Meta AI çš„ç ”ç©¶ç§‘å­¦å®¶ Kai Sheng Tai | ç¨€ç–æ€§ä¿ƒè¿›é«˜æ•ˆ LLM æ¨ç†](https://www.youtube.com/watch?v=lIuHPxsgymU)

- [è‹±ä¼Ÿè¾¾æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸“å®¶ Christian Merkwirth | ä¼˜åŒ– LLM æ¨ç†ï¼šæŒ‘æˆ˜ä¸æœ€ä½³å®è·µ](https://www.youtube.com/watch?v=f7XcHUwQl4Y)

- [è‹±ä¼Ÿè¾¾é«˜çº§æ•°æ®ç§‘å­¦å®¶ Mark Moyou | ä»ç†è®ºåˆ°å…·æœ‰æˆæœ¬æ•ˆç›Šçš„éƒ¨ç½²ï¼ŒæŒæ¡ LLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=9tvJ_GYJA-o)

- [NVIDIA GTC æŠ€æœ¯å¹²è´§ | è¶…å¤§å‹ Transformer æ¨¡å‹çš„é«˜æ•ˆæ¨ç†](https://www.nvidia.cn/on-demand/session/gtcspring23-s51088/)

- [ç¡…è°·çŸ¥ååäºº AI ç§‘å­¦å®¶ç”°æ¸Šæ ‹ | æ”¯æŒé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡çš„ LLMs çš„é«˜æ•ˆæ¨ç†](https://www.youtube.com/watch?v=eXPhvQgAT_I)

- [Julien Simon æ›¾ä»»äºšé©¬é€Šï¼ˆAWSï¼‰äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ å…¨çƒå¸ƒé“å¸ˆ | æ·±åº¦æ¢ç©¶ï¼šLLM æ¨ç†ä¼˜åŒ–](https://www.youtube.com/watch?v=hMs8VNRy5Ys)

- [OneFlow - å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†](https://www.bilibili.com/video/BV1pH4y1B7tY/)

- [KTransformers å›¢é˜Ÿåˆ†äº«å¼‚æ„æ¨ç†æ¶æ„æ€è·¯ï¼šåŸºäºå†…å­˜çš„å¤§æ¨¡å‹æ¨ç†æˆæœ¬ä¼˜åŒ–](https://www.bilibili.com/video/BV1VNQrYGEad/)


## ğŸ› å¼€æºé¡¹ç›®

å¼€æºé¡¹ç›®ï¼š

- [vLLM - Easy, Fast, and Cheap LLM Serving for Everyone](https://github.com/vllm-project/vllm)

- [SGLang - A Fast Serving Framework for Large Language Models and Vision Language Models](https://github.com/sgl-project/sglang)

- [DeepSeek - The Path to Open-Sourcing the DeepSeek Inference Engine](https://github.com/deepseek-ai/open-infra-index)

- [HUAWEI Ascend MindIE](https://www.hiascend.com/software/mindie)

- [llama.cpp - LLM Inference in C/C++](https://github.com/ggml-org/llama.cpp)

- [stable-diffusion.cpp - Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All Devices](https://github.com/SealAILab/stable-diffusion-cpp)


## ğŸ“ åšå®¢

- [ç»“åˆ MindIE-LLM æ¡†æ¶çš„å…·ä½“ä¼˜åŒ–æ¡ˆä¾‹ï¼Œåˆ†æå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿçš„å…³é”®æŠ€æœ¯](https://mp.weixin.qq.com/s/3QYQDq4ZHQRwYMs6MmgVLg)

- [ä¸¤ä¸‡å…­åƒå­—ï¼Œå¤§æ¨¡å‹æ ¸å¿ƒæŠ€æœ¯ç»¼è¿°ï¼šå¾®è°ƒã€æ¨ç†ä¸ä¼˜åŒ–æŒ‡å—](https://mp.weixin.qq.com/s/TCG_dDhoUvtlmtcO_dwSgw)

- [å¤§æ¨¡å‹æ¨ç†æœåŠ¡å…¨æ™¯å›¾](https://mp.weixin.qq.com/s/cDELflSEM7SV2Z3bupy65g)

- [Lilian Weng å‡ºå“ï¼Œå¿…æ˜¯ç²¾å“ | å¤§å‹ Transformer æ¨¡å‹æ¨ç†ä¼˜åŒ–](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

- [é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶çš„è®¾è®¡ä¸å®ç°](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

- [AI æ¨ç†åœºæ™¯çš„ç—›ç‚¹å’Œè§£å†³æ–¹æ¡ˆ](https://mp.weixin.qq.com/s/SeUJxNK10fhR6YsWSJRYwg)

- [Semi-PDï¼ŒP/D åŠåˆ†ç¦»çš„è°ƒåº¦ç­–ç•¥ï¼Œå¤§æ¨¡å‹æ¨ç†èŒƒå¼æ–°é€‰æ‹©](https://mp.weixin.qq.com/s/vQ5iXCXD7lJXogvT52PsLg)

- [ä¸€å¿µ LLM å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿ](https://mp.weixin.qq.com/s/bmafuEaB3pfG72xEaPcR3g)

- [2024 å¹´ - å¼€æºå¤§æ¨¡å‹æ¨ç†å¼•æ“ç°çŠ¶åŠå¸¸è§æ¨ç†ä¼˜åŒ–æ–¹æ³•](https://zhuanlan.zhihu.com/p/755874470)

- [å¤§æ¨¡å‹æ¨ç†å¼•æ“å›½äº§åŒ–æ€è€ƒå’Œå®è·µ](https://mp.weixin.qq.com/s/JLeYxYXurHbtF0nL-2wYyw)

- [å¤§æ¨¡å‹çš„æ¨¡å‹å‹ç¼©ä¸æœ‰æ•ˆæ¨ç†è¦ç‚¹æ€»ç»“](https://mp.weixin.qq.com/s/8AltJXjXIZHvq7lPu8FKoQ)

- [ã€Œä¹˜æ³•å˜åŠ æ³•ã€ï¼MIT æ¸…åæ ¡å‹å…¨æ–°æ–¹æ³•ä¼˜åŒ– Transformerï¼šAddition is All You Need](https://mp.weixin.qq.com/s/LdRacBGfjyF8xUJP6h6xbw)


## ğŸ’» æ–¹æ³•è®ºæ–‡

- [Li, Y., Wei, F., Zhang, C., & Zhang, H. (2025). EAGLE-3: Scaling Up Inference Acceleration of Large Language Models via Training-Time Test. arXiv preprint arXiv: 2503.01840.](https://arxiv.org/abs/2503.01840)

- [Zhang, T., Sui, Y., Zhong, S., Chaudhary, V., Hu, X., & Shrivastava, A. (2025). 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. arXiv preprint arXiv: 2504.11651.](https://arxiv.org/abs/2504.11651)

- [Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., ... & Ceze, L. (2025). FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving. arXiv preprint arXiv: 2501.01005.](https://arxiv.org/abs/2501.01005)

- [Prabhu, R., Nayak, A., Mohan, J., Ramjee, R., & Panwar, A. (2025). vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 1: 1133-1150.](https://doi.org/10.1145/3669940.3707256)

- [Huang, Y., Wan, L. J., Ye, H., Jha, M., Wang, J., Li, Y., ... & Chen, D. (2024). Invited: New Solutions on LLM Acceleration, Optimization, and Application. In Proceedings of the 61st ACM/IEEE Design Automation Conference (DAC), 1-4.](https://doi.org/10.1145/3649329.3663517)

- [Chen, Y., & Huang, G. (2024). GUIDE: A Global Unified Inference Engine for Deploying Large Language Models in Heterogeneous Environments. arXiv preprint arXiv: 2412.04788.](https://arxiv.org/abs/2412.04788)

- [Dong, J., Feng, B., Guessous, D., Liang, Y., & He, H. (2024). Flex Attention: A Programming Model for Generating Optimized Attention Kernels. arXiv preprint arXiv: 2412.05496.](https://arxiv.org/abs/2412.05496)

- [Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., ... & Stoica, I. (2023). Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP), 611-626.](https://doi.org/10.1145/3600006.3613165)


## ğŸ’¡ ç»¼è¿°è®ºæ–‡

- [Zhen, R., Li, J., Ji, Y., Yang, Z., Liu, T., Xia, Q., ... & Zhang, M. (2025). Taming the Titans: A Survey of Efficient LLM Inference Serving. arXiv preprint arXiv: 2504.19720.](https://arxiv.org/abs/2504.19720)

- [Park, S., Jeon, S., Lee, C., Jeon, S., Kim, B. S., & Lee, J. (2025). A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency. arXiv preprint arXiv: 2505.01658.](https://arxiv.org/abs/2505.01658)

- [Liu, Y., Wu, J., He, Y., Gao, H., Chen, H., Bi, B., ... & Hooi, B. (2025). Efficient Inference for Large Reasoning Models: A Survey. arXiv preprint arXiv: 2503.23077.](https://arxiv.org/abs/2503.23077)

- [Zhou, Z., Ning, X., Hong, K., Fu, T., Xu, J., Li, S., ... & Wang, Y. (2024). A Survey on Efficient Inference for Large Language Models. arXiv preprint arXiv: 2404.14294.](https://arxiv.org/abs/2404.14294)

- [Chavan, A., Magazine, R., Kushwaha, S., Debbah, M., & Gupta, D. (2024). Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI), 7980-7988.](https://doi.org/10.24963/ijcai.2024/883)

- [Luohe, S., Zhang, H., Yao, Y., Li, Z & Zhao, H. (2024). Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption. In First Conference on Language Modeling (COLM).](https://openreview.net/forum?id=8tKjqqMM5z#discussion)

- [Li, H., Li, Y., Tian, A., Tang, T., Xu, Z., Chen, X., ... & Chen, L. (2024). A Survey on Large Language Model Acceleration Based on KV Cache Management. arXiv preprint arXiv: 2412.19442.](https://arxiv.org/abs/2412.19442)

- [Kim, S., Hooper, C., Wattanawong, T., Kang, M., Yan, R., Genc, H., ... & Gholami, A. Full Stack Optimization of Transformer Inference. In Architecture and System Support for Transformer Models (ASSYST@ ISCA 2023).](https://arxiv.org/abs/2302.14017)

